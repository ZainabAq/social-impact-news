---
title: "main-file"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Remains the same for everyone
```{r}
library(readr)
# change this according to your
data_10000 <- read_csv("Raw data_10000 articles_Jul-Dec 2017.csv")
```


# Sections for everyone

# Minji

```{r}
library(dplyr)
library(mosaic)
library(randomForest)
library(ParetoPosStable)
library(tree)
library(caret)
library(gbm)
library(adabag)
library(EnvStats)
library(Hmisc)
library(tidyr)

##removing na's and creating the existing metric
Sint_PV <- data_10000%>%
  mutate(metric = `Social interactions`/Views)%>%
  arrange(desc(metric))%>%
  na.omit(`Engaged minutes`)%>%
  na.omit(`Social interactions`)

##creating vectors 
engaged_min <- Sint_PV$`Engaged minutes`
metric <-Sint_PV$metric
soc_int <- Sint_PV$`Social interactions`
pv <-as.data.frame(Sint_PV$`Views`) 
V <-Sint_PV$`Views`
rv <- Sint_PV$`Returning vis.`


##Pareto Distribution
pareto.fit(V, estim.method = "MLE")

ggplot(pv, aes(log(V))) + geom_histogram(aes(y=..density..), bins = 100 ) + stat_function(fun = dnorm, args = list(11.68749, 1.013802), color = "red")

p <- ggplot(pv, aes(V)) + geom_histogram(aes(y=..density..), bins = 100) 
p+ stat_function(fun = dpareto, args = list(7795, 0.4751), color = "red")

###################################################################################################

merged_200 <- read_csv("merged_200.csv")
minji_50 <- merged_200[101:150,]
write.table(minji_50, file="minji_50.csv",sep=",",row.names=F)
minji_50_impact <- read_csv("minji_50_impact.csv")
rated_200 <- read_csv("rated_200.csv")
tally(~Section, data_10000)
politics <- data_10000 %>%
  filter(Section == "Politics")
politics <- politics[151:225,]
write.table(politics, file="politics.csv", sep =",", row.names=F)

#politics_75 <-read_csv("text-data-politics.csv")
#politics_75 <- politics_75[151:225,]
#write.table(politics_75, file="politics_75.csv", sep=",",row.names=F)
####################################################################################################################

#politics_75 <-read_csv("text-data-politics.csv")
#politics_75 <- politics_75[151:225,]
#write.table(politics_75, file="politics_75.csv", sep=",",row.names=F)
###################################################################################################


rated_200 <- rated_200 %>%
  mutate(Impact1 = ifelse(Impact == 9, 1, ifelse(Impact == 1, 1, 0)))%>%
  select(-Impact)


##Cleaning Data before Modeling 
spaceless <- function(x) {colnames(x) <- gsub(" ", "_", colnames(x));x}
hless <- function(x) {colnames(x) <- gsub("-", "_", colnames(x));x}
clean_200 <- spaceless(rated_200)
clean_200 <- hless(clean_200)
clean_200 <- na.omit(clean_200)

##Classification Tree of the whole dataset
tree <- tree(as.factor(Impact1) ~. -Tags -`Sort_(Views)` -text -keywords -summary -URL -Title -Authors -Section -Publish_date -X1 , clean_200 )
head(clean_200)
summary(tree)
plot(tree)
text(tree, pretty=0)

#Dividing the data into testing and training
set.seed(5)
clean_train <- clean_200 %>%
  sample_frac(0.5)

clean_test <- clean_200%>%
  setdiff(clean_train)

##Random Forest model 
set.seed(5)
m0 <- randomForest(as.factor(Impact1) ~. -Tags -`Sort_(Views)` -text -keywords -summary -URL -Title -Authors -Section -Publish_date -X1, data=clean_train, importance = TRUE, ntree=2000, mtry=5)
varImpPlot(m0)

m0_estimates = predict(m0, newdata = clean_test, n.trees = 2000) 

table(m0_estimates, clean_test$Impact1)

(17+23)/64 ##60.9% correct predictions.


##Random Forest model with Conditional inference (uses statistical test rather than purity)
set.seed(5)
m1 <- randomForest(as.factor(Impact1) ~. -Tags -`Sort_(Views)` -text -keywords -summary -URL -Title -Authors -Section -Publish_date -X1, data=clean_train, controls=cforest_unbiased(ntree=2000, mtry=5))
varImpPlot(m1)

m1_estimates = predict(m1, newdata = clean_test, OOB=TRUE, type = "response") 
table(m1_estimates, clean_test$Impact1)
(15+22)/64 ##62.5 correct predictions

##Classication tree
set.seed(5)
m2 <- tree(as.factor(Impact1) ~ . -Tags -`Sort_(Views)` -text -keywords -summary -URL -Title -Authors -Section -Publish_date -X1, data=clean_train)
summary(m2)
plot(m2)
text(m2, pretty = 0)
m2_estimates = predict(m2, clean_test, type = "class")
table(m2_estimates, clean_test$Impact1)
(22+19)/64 ##64.1% correct predictions 

##Pruned tree with CV 
set.seed(5)
m3 <- cv.tree(m2, FUN = prune.misclass)
plot(m3$size, m3$dev, type = "b")
m4 <- prune.misclass(m2, best = 5)
plot(m4)
text(m4, pretty = 0)
m4_estimates = predict(m4, clean_test, type = "class")
table(m4_estimates, clean_test$Impact1)
(22+21)/64 ##67.2% correct predictions

##Boosting with Classification 
clean_boost <- clean_200 %>%
  select( -Tags, -`Sort_(Views)`, -text, -keywords, -summary, -URL, -Title, -Authors, -Staection, -Publish_date, -X1)

m5<- gbm(Impact1 ~., data = clean_boost, distribution = "bernoulli", n.trees = 500, cv.folds = 5, verbose = F)
best.iter = gbm.perf(m5, method = "cv")

clean_boost2 <- clean_boost %>%
  mutate(Impact = ifelse(Impact1 == 1, "Y", "N" )) %>%
  select(-Impact1)

m7 <- gbm(Impact1 ~., data = clean_boost, distribution = "bernoulli", cv.folds = 5, verbose = F, shrinkage = 0.01, interaction.depth = 1, n.trees = best.iter, n.minobsinnode = 1)
summary(m7)

set.seed(5)
fitControl = trainControl(method="cv", summaryFunction =  twoClassSummary, classProbs = TRUE, number = 5, returnResamp = "all")
m6 <- train(Impact~., data=clean_boost2, method="gbm", trControl=fitControl, tuneGrid=data.frame(.n.trees=best.iter, .shrinkage=0.01, .interaction.depth=1, .n.minobsinnode=1), verbose=FALSE,  metric = "ROC", distribution = "bernoulli")
m6
confusionMatrix(m6)
confusionMatrix(m6_estimates, as.factor(clean_boost2$Impact))
m6_estimates = predict(m6, clean_boost2)
postResample(m6_estimates, as.factor(clean_boost2$Impact))


##Accuracy 0.859, avg = 0.74


#####################################################################################################################

##Merging Data
politics_300 <- read_csv("text-stats-politics-300.csv")
politics <- data_10000 %>%
  filter(Section == "Politics")
politics_4 <- politics[401:450,]
write.table(politics_4, file="politics_401_450.csv", sep =",", row.names = F)
politics_300 <- left_join(politics_300, politics_2, by = "URL")
write.table(politics_300, file="politics-300-merged.csv", sep =",", row.names=F)
politics_300 <- politics_300 %>% 
  na.omit()%>% 
  select(-ID, -URL, -text, -`Publish date`, -Authors, -Section, -Tags, -`Sort (Views)` , -Title)
write.table(politics_300, file="politics-no-text-merged.csv", sep =",", row.names=F)

##POLITICS 900
politics_s3_text <- read_csv("text-data-nlp-politics.csv")
politics_s3_text = politics_s3_text[1:901,]
politics_s3 <- politics[1:901,]
politics_s3 <- left_join(politics_s3, politics_s3_text, by = "URL")
added_sent <- read_csv("politics_added_sentiment.csv")
added_sent = added_sent %>%
  select(URL, neg1, neg2, neg3, neg4, neg5, pos1, pos2, pos3, pos4, pos5)
added_sent = added_sent[1:901,]
politics_s3 <- left_join(politics_s3, added_sent, by = "URL")
politics_s3_rated <- read_csv("politics_rated_for_s3.csv")
politics_s3 <- left_join(politics_s3_rated, politics_s3, by ="URL")
politics_s3 = politics_s3 %>%
  select( -text, -`Publish date`, -Authors, -Section, -Tags, -`Sort (Views)`)
politics_s3 = politics_s3 %>%
  mutate(Other_Social_ref = `Social refs` - (`Fb refs`+`Tw refs`))%>%
  mutate(Li_ref = as.factor(ifelse(is.na(`Li refs`), 0, 1))) %>% 
  mutate(Pi_ref = as.factor(ifelse(is.na(`Pi refs`), 0, 1))) %>% 
  mutate(Other_int = `Social interactions` - (`Fb interactions` + `Tw interactions`)) %>%
  mutate(Li_int = as.factor(ifelse(is.na(`Li interactions`), 0 , 1))) %>%
  mutate(Pi_int = as.factor(ifelse(is.na(`Pi interactions`), 0 , 1)))

politics_s3 = politics_s3 %>%
  select( -`Li refs`, -`Pi refs`, -`Li interactions`, -`Pi interactions`, -ID, -`Social interactions`, -`Social refs`)

NA_find <-as.data.frame(is.na(politics_s3))

politics_s3 <- politics_s3[-c(279, 281, 680, 752, 763, 775),]

dulplicated <- as.data.frame(duplicated(politics_s3) | duplicated(politics_s3, fromLast = TRUE))

politics_s3 <- politics_s3[-c(51, 151, 298, 349, 782, 853),]

##Cleaning data 
spaceless <- function(x) {colnames(x) <- gsub(" ", "_", colnames(x));x}
hless <- function(x) {colnames(x) <- gsub("-", "_", colnames(x));x}
politics_s3 <- spaceless(politics_s3)
politics_s3 <- hless(politics_s3)
names(politics_s3)
write.table(politics_s3, file="politics-data-for-model.csv", sep = ",", row.names= F)
#politics_300 <- politics_300 %>%
  #mutate(metric = Social_interactions/Views)

##Classification Tree of the whole dataset
tree_p <- tree(as.factor(impact) ~. -URL-Title, politics_s3)
summary(tree_p)
plot(tree_p)
text(tree_p, pretty=0)
	
#Dividing the data into testing and training
politics_s3 <- politics_s3[-441,]

set.seed(1)
politics_train <- politics_s3 %>%
  sample_frac(0.5)

politics_test <- politics_s3%>%
  setdiff(politics_train)

##Classication tree
set.seed(5)
mp0 <- tree(as.factor(impact) ~ . -URL -Title, data=politics_train)
summary(mp0)
plot(mp0)
text(mp0, pretty = 0)
mp0_estimates = predict(mp0, politics_test, type = "class")
table(mp0_estimates, politics_test$impact)
(178+107)/444 ##64% correct predictions 


##Pruned tree with CV 
set.seed(5)
mp1 <- cv.tree(mp0, FUN = prune.misclass)
plot(mp1$size, mp1$dev, type = "b")
mp2 <- prune.misclass(mp0, best = 5)
plot(mp2)
text(mp2, pretty = 0)
mp2_estimates = predict(mp2, politics_test, type = "class")
table(mp2_estimates, politics_test$impact)
(234+77)/444 ##70% correct predictions

##Random Forest model 
set.seed(5)
mp3 <- randomForest(as.factor(impact) ~.-URL -Title,data=politics_train, importance = TRUE, ntree=2000, mtry=5)
varImpPlot(mp3)
mp3_estimates = predict(mp3, newdata = politics_test, n.trees = 2000) 
table(mp3_estimates, politics_test$impact)
(236+80)/444 ##71.2% correct predictions


##Random Forest model with Conditional inference (uses statistical test rather than purity)
set.seed(5)
mp4 <- randomForest(as.factor(impact) ~.-URL -Title , data=politics_train, controls=cforest_unbiased(ntree=2000, mtry=5))
varImpPlot(mp4)
mp4_estimates = predict(mp4, newdata = politics_test, type = "response") 
table(mp4_estimates, politics_test$impact)
(238+81)/444 ## 71.8% correct predictions



##Boosting with Classification 
mp5<- gbm(impact ~ . -URL -Title, data = politics_boost_train, distribution = "bernoulli", n.trees = 2000, cv.folds = 5, verbose = F)
best.iter = gbm.perf(mp5, method = "cv")

politics_boost <- politics_s3 %>%
  mutate(Impact = as.factor(ifelse(impact == 1, "Y", "N" ))) %>%
  select(-impact)

set.seed(5)
politics_boost_train <- politics_boost %>% 
  sample_frac(0.5)

politics_boost_test <- politics_boost %>% 
  setdiff(politics_boost_train)
  

mp7 <- gbm(impact ~ . -URL -Title , data = politics_s3, distribution = "bernoulli", cv.folds = 5, verbose = F, shrinkage = 0.01, interaction.depth = 1, n.trees = best.iter, n.minobsinnode = 1)
summary(mp7)


tune = expand.grid(interaction.depth = c(1, 2, 3),
                    n.trees = (0:50)*50, 
                    shrinkage = c(.1 ,.01, .001),
                    n.minobsinnode = (1:10))

set.seed(5)
fitControl = trainControl(method="cv", summaryFunction =  twoClassSummary, classProbs = TRUE, number = 5, returnResamp = "all")
mp6 <- train(Impact~.-URL -Title, data=politics_boost, method="gbm", trControl=fitControl, tuneGrid = tune, verbose=FALSE,  metric = "ROC", distribution = "bernoulli")
mp6
varImp(mp6)


confusionMatrix(mp6)
predictions <- predict(object=mp6, politics_boost, type='raw')
mp6_estimates = predict(mp6, politics_boost)
confusionMatrix(mp6_estimates, as.factor(politics_boost$Impact))
confusionMatrix(predictions,  as.factor(politics_boost$Impact))
##Accuracy 81.8% CI (0.788, 0.841) Kappa #0.589
postResample(predictions, as.factor(politics_boost$Impact))
postResample(mp6_estimates, as.factor(politics_boost$Impact))


# probabilites 
library(pROC)
predictions <- predict(object=mp6, politics_boost$Impact, type='prob')
head(predictions)

#AUC socre
auc = roc(ifelse(politics_boost[,44] == "Y", 1, 0), predictions[[2]])
print(auc$auc)
 
plot(varImp(mp6), nbars = 10)

save(mp6, file = "boosting.rda")
save(mp7, file = "boosting2.rda")
```

# Erina

# Erina

```{r}
#join manually rated impact scores
MK <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/minji_50_impact.csv") %>%
  select(-X1) #import original data
JA <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/julianna_50_impact.csv")  %>%
  select(-X1)#import original data
ZA <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/zainab_50_impact.csv")  %>%
  select(-X1)#import original data
EF <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/erina_50_impact.csv") %>%
  select(-X1) #import original data

MK$`Publish date` = as.character(MK$`Publish date`)
JA$`Publish date` = as.character(JA$`Publish date`)
ZA$`Publish date` = as.character(ZA$`Publish date`)
EF$`Publish date` = as.character(EF$`Publish date`)

rated_200 <- rbind(MK, JA)
rated_200 <- rbind(rated_200,ZA)
rated_200 <- rbind(rated_200,EF)

write.csv(rated_200, file = "C:/Users/choco/Desktop/SPRING 2018/Capstone/rated_200.csv")

MK2 <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/minji_75_impact.csv")
JA2 <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/julianna_75_impact.csv")
ZA2 <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/zainab_75_impact.csv")
EF2 <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/erina_75_impact.csv")


textual <- read_csv("C:/Users/choco/Desktop/SPRING 2018/Capstone/text-data-politics (3).csv") %>%
  select(c(URL, Text))

MK2 <- left_join(MK2, textual, by="URL")
MK2 <- MK2 %>%
  select(-Text.x) %>%
  rename(Text = Text.y)

MK2 <- MK2[c(2,1,3)]
# data_1000_EF<- read_csv("Raw data_10000 articles_Jul-Dec 2017.csv")
# names(data_1000_EF)
# lm1 <- lm(log(`Engaged minutes`)~.-URL-Title-Authors-Tags,data=data_1000_EF)
# summary(lm1)
# anova(lm1)
# 
# plot(lm1)
# summary(data_1000_EF)

library(readr)
library(tidytext)
library(dplyr)
library(stringr)

data_EF <- read_csv("Raw data_10000 articles_Jul-Dec 2017.csv") #import original data

data("stop_words") #import stop words
text <- read_csv("text-data.csv") %>%
  select(URL, article) #import textual data

text$article<- str_replace_all(text$article, "[[:punct:]]", "") #omit special characters
afinn <- get_sentiments("afinn")

afinn_neg5 <- get_sentiments("afinn") %>%
    filter(score == -5)
afinn_neg4 <- get_sentiments("afinn") %>%
    filter(score == -4)
afinn_neg3 <- get_sentiments("afinn") %>%
    filter(score == -3)
afinn_neg2 <- get_sentiments("afinn") %>%
    filter(score == -2)
afinn_neg1 <- get_sentiments("afinn") %>%
    filter(score == -1)
afinn_0 <- get_sentiments("afinn") %>%
    filter(score == 0)
afinn_pos1 <- get_sentiments("afinn") %>%
    filter(score == 1)
afinn_pos2 <- get_sentiments("afinn") %>%
    filter(score == 2)
afinn_pos3 <- get_sentiments("afinn") %>%
    filter(score == 3)
afinn_pos4 <- get_sentiments("afinn") %>%
    filter(score == 4)
afinn_pos5 <- get_sentiments("afinn") %>%
    filter(score == 5)


words <- text %>%
#  group_by(URL) %>%
  unnest_tokens(word, article)

words <- words %>%
  anti_join(stop_words)

neg5 <- words %>%
  group_by(URL) %>%
  inner_join(afinn_neg5) %>%
	inner_join(output) %>%
	count(word, sort=TRUE)

neg4 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_neg4) %>%
	count(word, sort=TRUE)

neg3 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_neg3) %>%
	count(word, sort=TRUE)

neg2 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_neg2) %>%
	count(word, sort=TRUE)

neg1 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_neg1) %>%
	count(word, sort=TRUE)

zero <- words %>%
  group_by(URL) %>%
	inner_join(afinn_0) %>%
	count(word, sort=TRUE)

neg1$score <- "-1"

pos1 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_pos1) %>%
	count(word, sort=TRUE)

pos2 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_pos2) %>%
	count(word, sort=TRUE)

pos3 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_pos3) %>%
	count(word, sort=TRUE)

pos4 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_pos4) %>%
	count(word, sort=TRUE)

pos5 <- words %>%
  group_by(URL) %>%
	inner_join(afinn_pos5) %>%
	count(word, sort=TRUE)

###at "2.2 Sentiment analysis with inner join"
```


```{r}
#####Erina continued



words %>%
	group_by(URL) %>%
	inner_join() %>%
	count(word, sort=TRUE)

words %>%
	group_by(URL) %>%
	inner_join(afinn_neg5) %>%
	count(word, sort=TRUE)

words <- words %>%
  group_by(URL) %>%
  mutate(proportion = n / sum(n))

words <- words%>%
  count(word, sort = TRUE) %>%
  #select(-n) %>% 
  spread(URL, proportion) %>% 
  gather(URL, proportion, 2:3)



#words %>%
 # group_by(article) %>%
  #count(word, sort = TRUE) 
ggplot2::ggplot(data = test, aes(`huff_metric`)) + geom_histogram()

ggplot(test, aes(`huff_metric`)) + 
    geom_histogram(aes(y=..density..),fill="gray40") +
    geom_density(alpha=.2, color = "red")

ggplot(data = test, aes(log(`huff_metric`))) + geom_histogram()

ggplot(test, aes(log(`huff_metric`))) + 
    geom_histogram(aes(y=..density..),fill="gray40") +
    geom_density(alpha=.2, color = "red")
  #testing normality
qqnorm(test$log_huff_metric)
qqline(test$log_huff_metric, col = "red")

#write.csv(text, file = "C:/Users/choco/Desktop/SPRING 2018/Capstone/text_cleanedinR.csv") #create new csv file
```

Social impact:change in way of thinking/attitude or brings someone to take action or change the way they act; leaves an impression or makes the reader think even if it doesn’t change his or her views
Quality Journalism:accurate, clear, multiple sources, relevance, change in way of thinking/attitude/action (includes above)
_Social impact can be a smaller part of quality journalism but not vice versa?_
https://www.huffingtonpost.com/2012/04/16/huffington-post-pulitzer-prize-2012_n_1429169.html


# Julianna 

```{r}
library(dplyr)
library(mosaic)
library(Hmisc)
library(car)
library(leaps)
library(tidyr)
library(data.table)
library(psych)

data_10000 %>%
  arrange(`Engaged minutes`) %>%
  select(Title)

ggplot(data = data_10000, aes(log(`Social interactions`))) + geom_histogram()

ggplot(data = data_10000, aes(log(`Views`))) + geom_histogram()

ggplot(data = data_10000, aes(`Views`)) + geom_histogram()

test <- data_10000 %>%
  mutate(`huff_metric` = `Social interactions`/ `Views`, log_huff_metric = log(huff_metric)) %>%
  arrange(desc(huff_metric))

fav_stats(test$log_huff_metric)

ggplot(data = test, aes(log(`huff_metric`))) + geom_histogram()

ggplot(data = test, aes(x = log(`Social interactions`), y = log(`Engaged minutes`))) + geom_point() + 
  geom_smooth()

test <- test %>%
  mutate(log_social_interactions = log(`Social interactions`), log_engaged_minutes = log(`Engaged minutes`), log_views = log(`Views`))

cor(test$log_social_interactions, test$log_engaged_minutes, use = "complete.obs")

qqnorm(test$log_huff_metric)
qqline(test$log_huff_metric, col = "red")

qqnorm(test$log_social_interactions)
qqline(test$log_social_interactions, col = "red")

qqnorm(test$Views)
qqline(test$Views, col = "red")

qqnorm(test$log_views)
qqline(test$log_views, col = "red")

##MODELS--------------------------------------------------

#Finding predictors from backward and stepwise regression
fullmodel <- lm(`huff_metric`~`Views`+`Engaged minutes`+`Returning vis.`+`New vis.`+`Social refs`+`Social interactions`+`Avg. minutes new vis.`+`Avg. minutes ret. vis.`+`Visitors`+`Avg. views new vis.`+`Avg. views ret. vis.`, data = test)

#Using log variables
fullmodel <- lm(log_huff_metric~`Views`+log_engaged_minutes+`Returning vis.`+`New vis.`+`Social refs`+log_social_interactions+`Avg. minutes new vis.`+`Avg. minutes ret. vis.`+`Visitors`+`Avg. views new vis.`+`Avg. views ret. vis.`, data = test)


# backward
fullMSE <- (summary(fullmodel)$sigma)^2
step(fullmodel, data = test, scale = fullMSE, direction = "backward")

test<-na.omit(test)
#stepwise
nullmodel <- lm(`huff_metric` ~ 1, data = test)
step(nullmodel, scope = list(upper = fullmodel),
scale = fullMSE, direction = "both")

#Stepwise gave me this model:
mod_huff <- lm(formula = huff_metric ~ `Social interactions` + `Avg. minutes ret. vis.` + 
    `Returning vis.` + `Engaged minutes` + `New vis.` + `Social refs` + 
    Views + `Avg. views new vis.`, data = test)

summary(mod_huff)

plot(mod_huff)

  #cor matrix for the model
selected_data_mod_step <- test %>%
  select(`Social interactions`, `Avg. minutes ret. vis.`, 
    `Returning vis.`, `Engaged minutes`, `New vis.`, `Social refs`, 
    Views, `Avg. views new vis.`)

  #Nothing abnormally high except (engaged min and returning visitors), (new vis and views), (Social refs and views)
res2 <- rcorr(as.matrix(selected_data_mod_step))
res2

  #checking VIF
vif(mod_huff)
#There are big issues of multicollinearity in this model

##How about best subset approach?

allsubset<-regsubsets(`huff_metric`~`Views`+`Engaged minutes`+`Returning vis.`+`New vis.`+`Social refs`+`Social interactions`+`Avg. minutes new vis.`+`Avg. minutes ret. vis.`+`Visitors`+`Avg. views new vis.`+`Avg. views ret. vis.`, data = test)
options(digits=3)

summary(allsubset)$adjr2
plot(allsubset,scale="adjr2")

summary(allsubset)$cp
plot(allsubset,scale="Cp")

#Using log variables
allsubset_log<-regsubsets(log_huff_metric~`Views`+`Engaged minutes`+`Returning vis.`+`New vis.`+`Social refs`+`Social interactions`+`Avg. minutes new vis.`+`Avg. minutes ret. vis.`+`Visitors`+`Avg. views new vis.`+`Avg. views ret. vis.`, data = test)
options(digits=3)

summary(allsubset_log)$adjr2
plot(allsubset_log,scale="adjr2")

summary(allsubset_log)$cp
plot(allsubset_log,scale="Cp")

#Gives us THIS model:
mod_all <- lm(`huff_metric`~`Views`+`Engaged minutes`+`Returning vis.`+`New vis.`+`Social interactions`+`Social refs`+`Avg. minutes ret. vis.`, data = test)

summary(mod_all)

#Log
mod_all_log <- lm(log_huff_metric~`Views`+`Returning vis.`+`New vis.`+`Social refs`+`Social interactions`+`Avg. minutes ret. vis.`+ `Avg. minutes new vis.`+Visitors, data = test)

summary(mod_all_log)

car::vif(mod_all_log)

plot(mod_all_log)

  #Checking for multicollinearity again
selected_data_mod_best <- test %>%
  select(`Views`,`Engaged minutes`,`Returning vis.`,`New vis.`,`Social refs`,`Social interactions`,`Avg. minutes ret. vis.`)

res2 <- rcorr(as.matrix(selected_data_mod_best))
res2

car::vif(mod_all)

plot(mod_all)
  #Way better with multicollinearity!

#checking section
mod_all_sec1 <- lm(`huff_metric`~`Views`+`Engaged minutes`+`Returning vis.`+`New vis.`+`Social refs`+`Social interactions`+`Avg. minutes ret. vis.`+ `Avg. views new vis.`+Section, data = test)

summary(mod_all_sec1)

test_comb_section <- test %>%
  mutate(black_voices = ifelse(Section == "Black Voices", "yes", "no"))
#what happens when I add that to the model?
mod_all_sec <- lm(`huff_metric`~`Views`+`Engaged minutes`+`Returning vis.`+`New vis.`+`Social refs`+`Social interactions`+`Avg. minutes ret. vis.`+ `Avg. views new vis.`+black_voices, data = test_comb_section)

#This should be something we consider. Do sections fit into theoretical definition of social impact?

##how about engaged minutes per viewer?
test <- test %>%
  mutate(engaged_min_per_view = `Engaged minutes`/Views, engaged_min_per_ret_vis = `Engaged minutes`/`Returning vis.`)%>%
  arrange(desc(engaged_min_per_ret_vis))

mosaic::favstats(test$engaged_min_per_view)

ggplot(test, aes(engaged_min_per_view)) + geom_histogram()

mosaic::favstats(test$engaged_min_per_ret_vis)

ggplot(test, aes(log(engaged_min_per_ret_vis))) + geom_histogram()

#Amazon Mechanical Turks Analysis

Turk <- read_csv("Batch_3145287_batch_results.csv")
######################################

Turk_time<-Turk %>%
  filter(WorkTimeInSeconds >= 29)

tidyTurk <- Turk %>% 
  mutate(y = ifelse(Answer.categories == "category 1", 0, 1)) %>%
  group_by(Input.URL) %>%
  summarise(ys = paste(y, collapse = ","))%>%
  separate(ys, into = c("y1", "y2", "y3"))

corTurk <- tidyTurk %>%
  select(y1, y2, y3)

check <- rcorr(as.matrix(corTurk))
check

#Majority rules
Turk2 <- Turk_time %>% 
  mutate(y = ifelse(Answer.categories == "category 1", 0, 1)) %>%
  select(Input.URL, y)

Turk2 <- data.table(Turk2)

Turk2 <- Turk2[,as.numeric(names(which.max(table(y)))),by=Input.URL]

Turk2 <- data.frame(Turk2)
tally(Turk2$V1)

#majority rules-- all 1
Turk3 <- tidyTurk %>%
  mutate(all1 = ifelse(y1 == 1 & y2 == 1 & y3 == 1, 1, 0))

tally(Turk3$all1)

#Corr.test?
tidyTurk$y1 <- as.numeric(tidyTurk$y1)
tidyTurk$y2 <- as.numeric(tidyTurk$y2)

corr.test(tidyTurk$y1, tidyTurk$y2)

#permuting

```


# Zainab

```{r}
require(mosaic)
favstats(data_10000$Views)

c <- ggplot(data_10000, aes(Views))
c+geom_histogram()

fav_stats(data_10000$`Engaged minutes`)
d <- ggplot(data_10000, aes(`Engaged minutes`))
d+geom_histogram()

ggplot(data_10000, aes_string(x = "`Social refs`", y = "`Engaged minutes`")) +
  theme_bw() +
  geom_jitter()

ggplot(data_10000, aes_string(x = "`Returning vis.`", y = "`Engaged minutes`")) +
  theme_bw() +
  geom_jitter()

ggplot(data_10000, aes_string(x = "Visitors", y = "`Engaged minutes`")) +
  theme_bw() +
  geom_jitter()

data_10000 = data_10000 %>%
  mutate(logEngMins = log(`Engaged minutes`))

data_10000 = data_10000 %>%
  mutate(logRetVisitors = log(`Returning vis.`))

ggplot(data_10000, aes_string(x = "logRetVisitors", y = "logEngMins")) +
  theme_bw() +
  geom_jitter()

c <- ggplot(data_10000, aes(logRetVisitors))
c+geom_histogram()

f <- ggplot(data_10000, aes(logEngMins))
f+geom_histogram()

ggplot(data_10000, aes_string(x = "`Social refs`", y = "Visitors")) +
  theme_bw() +
  geom_jitter()

ggplot(data_10000, aes_string(x = "`Social refs`", y = "`New vis.`")) +
  theme_bw() +
  geom_jitter()


data_10000 %>%
  select(c(`Returning vis.`,`Social refs`, `Engaged minutes`, `Avg. views new vis.`, Views, Visitors, `New vis.`))%>%
  cor()
```


```{bash}
python3 parser.py data/politics.csv
```



